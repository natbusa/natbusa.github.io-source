{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parquet Pushdown Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filters can be applied to parquet files to reduce the volume of the data loaded. In particular parquet objects support partition filters and regular row filtering. Spark dags if proprerly constructed can push down some of the filters to the parquet object reader. Here below you will fine a number of test cases when this works correctly and a number of scenario's where filters pushdown does not apply.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datafaucet as dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = dfc.engine('spark')\n",
    "spark = engine.context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a sample dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>g</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   g    id\n",
       "0  0  2520\n",
       "1  1  2544\n",
       "2  3  2432\n",
       "3  2  2504"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dfc.range(10000).cols.create('g').randchoice([0,1,2,3])\n",
    "df.cols.groupby('g').agg('count').data.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data as a parquet object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.repartition('g').save('local', 'groups.parquet');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>g=2</td>\n",
       "      <td>DIRECTORY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>g=1</td>\n",
       "      <td>DIRECTORY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>g=3</td>\n",
       "      <td>DIRECTORY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>g=0</td>\n",
       "      <td>DIRECTORY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>_SUCCESS</td>\n",
       "      <td>FILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>._SUCCESS.crc</td>\n",
       "      <td>FILE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name       type\n",
       "0            g=2  DIRECTORY\n",
       "1            g=1  DIRECTORY\n",
       "2            g=3  DIRECTORY\n",
       "3            g=0  DIRECTORY\n",
       "4       _SUCCESS       FILE\n",
       "5  ._SUCCESS.crc       FILE"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfc.list('data/save/groups.parquet').data.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data parquet objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dfc.load('data/save/groups.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Debugging the physical query plan\n",
    "\n",
    "Here below we are going to debug the query plan. This can be done with the dataframe method `.explain()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep things simple let's focus only on the Parquet File Reader. In particular the function `explainSource(obj)` here below parses and prints out only some of the file reader parameters relevant for parquet filter and partition filter pushdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explainSource(obj):\n",
    "    for s in obj._jdf.queryExecution().simpleString().split('\\n'):\n",
    "        if 'FileScan' in s:\n",
    "            params = [\n",
    "                'Batched', \n",
    "                'Format', \n",
    "                'Location',\n",
    "                'PartitionCount', \n",
    "                'PartitionFilters', \n",
    "                'PushedFilters',\n",
    "                'ReadSchema']\n",
    "            \n",
    "            # (partial) parse the Filescan string\n",
    "            res = {}\n",
    "            # preamble\n",
    "            first, _, rest = s.partition(f'{params[0]}:')\n",
    "            # loop\n",
    "            for i in range(len(params[1:])):\n",
    "                first, _, rest = rest.partition(f'{params[i+1]}:')\n",
    "                res[params[i]]=first[1:-2]\n",
    "            # store last\n",
    "            res[params[-1]]=rest[1:]\n",
    "            \n",
    "            # hide location data, not relevant here\n",
    "            del res['Location']\n",
    "            \n",
    "            return dfc.yaml.YamlDict(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Pushdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first test does not filter anything. However as you see the partitionj variable `g` is materialized in directories and does not appear in the readSchema, which only includes those columns which are not partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batched: 'true'\n",
       "Format: Parquet\n",
       "PartitionCount: '4'\n",
       "PartitionFilters: '[]'\n",
       "PushedFilters: '[]'\n",
       "ReadSchema: struct<id:bigint>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### No pushdown on the physical plan\n",
    "explainSource(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting does not require any column, therefore the next one effectely just count data-less rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batched: 'true'\n",
       "Format: Parquet\n",
       "PartitionCount: '4'\n",
       "PartitionFilters: '[]'\n",
       "PushedFilters: '[]'\n",
       "ReadSchema: struct<>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Pushdown only column selection\n",
    "res = df.groupby('g').count()\n",
    "explainSource(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering on a column which is not a partition triggers a columnar filter during read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batched: 'true'\n",
       "Format: Parquet\n",
       "PartitionCount: '4'\n",
       "PartitionFilters: '[]'\n",
       "PushedFilters: '[IsNotNull(id), GreaterThan(id,100)]'\n",
       "ReadSchema: struct<id:bigint>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# push down row filter only but take all partitions\n",
    "res = df.filter('id>100')\n",
    "explainSource(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filters can be combined. For example here below a partition and a row (columnar) filter are part of the same filter statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batched: 'true'\n",
       "Format: Parquet\n",
       "PartitionCount: '1'\n",
       "PartitionFilters: '[isnotnull(g#92), (g#92 = 1)]'\n",
       "PushedFilters: '[IsNotNull(id), GreaterThan(id,100)]'\n",
       "ReadSchema: struct<id:bigint>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pushdown partition filters and row (columnar) filters\n",
    "res = df.filter('id>100 and g=1').groupby('g').count()\n",
    "explainSource(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filters can combined with logical operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batched: 'true'\n",
       "Format: Parquet\n",
       "PartitionCount: '2'\n",
       "PartitionFilters: '[((g#265 = 2) || (g#265 = 3))]'\n",
       "PushedFilters: '[IsNotNull(id), GreaterThan(id,100)]'\n",
       "ReadSchema: struct<id:bigint>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pushdown partition filters and row (columnar) filters\n",
    "res = df.filter('id>100 and (g=2 or g=3)').groupby('g').count()\n",
    "explainSource(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition filters can also be greater-than and less-than predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batched: 'true'\n",
       "Format: Parquet\n",
       "PartitionCount: '2'\n",
       "PartitionFilters: '[isnotnull(g#265), (g#265 > 1)]'\n",
       "PushedFilters: '[IsNotNull(id), GreaterThan(id,100)]'\n",
       "ReadSchema: struct<id:bigint>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pushdown partition filters and row (columnar) filters\n",
    "res = df.filter('id>100 and g>1').groupby('g').count()\n",
    "explainSource(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filters can be heaped up and cascaded. Effectively adding more filters will `AND`'ed together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batched: 'true'\n",
       "Format: Parquet\n",
       "PartitionCount: '1'\n",
       "PartitionFilters: '[isnotnull(g#265), (g#265 > 1), (g#265 = 2)]'\n",
       "PushedFilters: '[IsNotNull(id), GreaterThan(id,100), LessThan(id,500)]'\n",
       "ReadSchema: struct<id:bigint>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pushdown partition filters and row (columnar) filters can be added up\n",
    "res = df.filter('id>100 and g>1').filter('id<500 and g=2').groupby('g').count()\n",
    "explainSource(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When pushdown filters are NOT applied.\n",
    "\n",
    "#### Avoid caching and actions of read data \n",
    "Avoid cache(), count() or other action on data, as they will act as a \"wall\" for filter operations to be pushed down the parquet reader. On the contrary, registering the dataframe as a temorary table is OK. Please be aware that these operation could be hidden in your function call stack, so be always sure that the filters are as close as possible to the read operation.\n",
    "\n",
    "#### Spark will only read the same data once per session\n",
    "Once a parquet file has been read in a cached/unfiltered way, any subsequent read operation will fail to push down the filters, as spark assumes that the data has already been loaded once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, g: int]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dfc.load('data/save/groups.parquet')\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batched: 'true'\n",
       "Format: Parquet\n",
       "PartitionCount: '4'\n",
       "PartitionFilters: '[]'\n",
       "PushedFilters: '[]'\n",
       "ReadSchema: struct<id:bigint>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pushdown partition filters and row (columnar) filters are ignored after cache, count, and the like\n",
    "res = df.filter('id>100 and g=1').groupby('g').count()\n",
    "explainSource(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-read will not push down the filters ...\n",
    "df = dfc.load('data/save/groups.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batched: 'true'\n",
       "Format: Parquet\n",
       "PartitionCount: '4'\n",
       "PartitionFilters: '[]'\n",
       "PushedFilters: '[]'\n",
       "ReadSchema: struct<id:bigint>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pushdown partition filters and row (columnar) filters are ignored after cache, count, and the like\n",
    "res = df.filter('id>100 and g=1').groupby('g').count()\n",
    "explainSource(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
