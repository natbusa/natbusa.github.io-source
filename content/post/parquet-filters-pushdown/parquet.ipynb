{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datafaucet as dfc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datafaucet is a productivity framework for ETL, ML application. Simplifying some of the common activities which are typical in Data pipeline such as project scaffolding, data ingesting, start schema generation, forecasting etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Saving Parquet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [datafaucet] NOTICE parquet.ipynb:engine:__init__ | Connecting to spark master: local[*]\n",
      " [datafaucet] NOTICE parquet.ipynb:engine:__init__ | Engine context spark:2.4.4 successfully started\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<datafaucet.project.Project at 0x7f3f40acb748>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfc.project.load('minimal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "profile: minimal\n",
       "variables: {}\n",
       "engine:\n",
       "    type: spark\n",
       "    master: local[*]\n",
       "    jobname:\n",
       "    timezone: naive\n",
       "    submit:\n",
       "        jars: []\n",
       "        packages: []\n",
       "        pyfiles:\n",
       "        files:\n",
       "        repositories:\n",
       "        conf:\n",
       "providers:\n",
       "    local:\n",
       "        service: file\n",
       "        path: data\n",
       "resources: {}\n",
       "logging:\n",
       "    level: info\n",
       "    stdout: true\n",
       "    file: datafaucet.log\n",
       "    kafka: []"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfc.metadata.profile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter and projections Filters push down on parquet files\n",
    "\n",
    "The following show how to selectively read files on parquet files (with partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>g</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2480</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   g    id\n",
       "0  0  2552\n",
       "1  1  2400\n",
       "2  3  2568\n",
       "3  2  2480"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = dfc.range(10000).cols.create('g').randchoice([0,1,2,3])\n",
    "df.cols.groupby('g').agg('count').data.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save data as parquet objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [datafaucet] INFO parquet.ipynb:engine:save_log | save\n"
     ]
    }
   ],
   "source": [
    "df.repartition('g').save('local', 'groups.parquet');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>g=2</td>\n",
       "      <td>DIRECTORY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>g=1</td>\n",
       "      <td>DIRECTORY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>g=3</td>\n",
       "      <td>DIRECTORY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>g=0</td>\n",
       "      <td>DIRECTORY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>_SUCCESS</td>\n",
       "      <td>FILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>._SUCCESS.crc</td>\n",
       "      <td>FILE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name       type\n",
       "0            g=2  DIRECTORY\n",
       "1            g=1  DIRECTORY\n",
       "2            g=3  DIRECTORY\n",
       "3            g=0  DIRECTORY\n",
       "4       _SUCCESS       FILE\n",
       "5  ._SUCCESS.crc       FILE"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfc.list('data/save/groups.parquet').data.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data parquet objects (with pushdown filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = dfc.engine().context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [datafaucet] INFO parquet.ipynb:engine:load_log | load\n"
     ]
    }
   ],
   "source": [
    "df = dfc.load('local', 'groups.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) FileScan parquet [id#253L,g#254L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/home/natbusa/Projects/datafaucet/examples/tutorial/groups.parquet/data], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint,g:bigint>\n"
     ]
    }
   ],
   "source": [
    "### No pushdown on the physical plan\n",
    "df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[g#254L], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(g#254L, 200)\n",
      "   +- *(1) HashAggregate(keys=[g#254L], functions=[partial_count(1)])\n",
      "      +- *(1) FileScan parquet [g#254L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/home/natbusa/Projects/datafaucet/examples/tutorial/groups.parquet/data], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<g:bigint>\n"
     ]
    }
   ],
   "source": [
    "### Pushdown only column selection\n",
    "df.groupby('g').count().explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [id#253L, g#254L]\n",
      "+- *(1) Filter (isnotnull(id#253L) && (id#253L > 100))\n",
      "   +- *(1) FileScan parquet [id#253L,g#254L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/home/natbusa/Projects/datafaucet/examples/tutorial/groups.parquet/data], PartitionFilters: [], PushedFilters: [IsNotNull(id), GreaterThan(id,100)], ReadSchema: struct<id:bigint,g:bigint>\n"
     ]
    }
   ],
   "source": [
    "# push down row filter only but take all partitions\n",
    "df.filter('id>100').explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[g#254L], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(g#254L, 200)\n",
      "   +- *(1) HashAggregate(keys=[g#254L], functions=[partial_count(1)])\n",
      "      +- *(1) Project [g#254L]\n",
      "         +- *(1) Filter (((isnotnull(id#253L) && isnotnull(g#254L)) && (id#253L > 100)) && (g#254L = 1))\n",
      "            +- *(1) FileScan parquet [id#253L,g#254L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/home/natbusa/Projects/datafaucet/examples/tutorial/groups.parquet/data], PartitionFilters: [], PushedFilters: [IsNotNull(id), IsNotNull(g), GreaterThan(id,100), EqualTo(g,1)], ReadSchema: struct<id:bigint,g:bigint>\n"
     ]
    }
   ],
   "source": [
    "# pushdown partition filters and row (columnar) filters\n",
    "df.filter('id>100 and g=1').groupby('g').count().explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
